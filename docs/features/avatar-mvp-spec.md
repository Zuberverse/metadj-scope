# MetaDJ Avatar MVP Spec (Scope)

**Last Modified**: 2025-12-30 17:31 EST
**Status**: Active (MVP in progress)

## Overview
Build a real-time MetaDJ avatar generated by Scope from live webcam input. Scope is the rendering engine; VACE locks identity; ControlNet guides pose (future). The demo must run reliably for the Jan 9 checkpoint.

## Goals
- Prove a Scope-generated avatar can embody MetaDJ identity in real time.
- Deliver a stable, demo-ready flow: webcam in, avatar out.
- Keep controls minimal and latency low.

## Non-Goals (MVP)
- TouchDesigner integration
- Advanced compositing or multi-layer pipelines
- Full-body tracking or multi-person tracking
- Perfect alpha/mask output (future)

## User Flow (MVP)
1. Open demo and focus Avatar Studio.
2. Start webcam.
3. Set prompt + VACE reference path.
4. Start stream.
5. Apply updates as needed without restarting.

## Functional Requirements

### P0 (Must Have)
- Start and stop a Scope stream.
- Provide VACE reference images to lock MetaDJ identity.
- Ingest webcam input via WebRTC video track (video-to-video mode).
- Render live avatar output with consistent identity.
- Show basic status (warming, live, error).
- Allow a Scope server asset path for VACE reference images.
- Apply prompt/VACE updates over the data channel without restarting the stream.
- Auto-reconnect on dropped connections (3 attempts with backoff).
- Require webcam to be active before starting the stream.

### P1 (Should Have)
- Prompt presets for MetaDJ styles (e.g., default, high-contrast, neon).
- Simple intensity control (guidance, delta, or ControlNet scale).
- Record a short demo capture.

## Technical Requirements

### Scope Pipeline (Baseline)
- Model: `longlive`
- Resolution: 320Ã—576 (matches current defaults)
- Denoising steps: `[1000, 750, 500, 250]`
- VACE: enabled when server asset path is provided
- Input mode: `video` (required for webcam video-to-video)
- ControlNet: OpenPose if supported; optional depth/edges if latency allows.

### VACE
- Reference set: curated MetaDJ avatar images (3-8 images).
- Validate input format (URL/base64/file) against Scope API.

### Dynamic Parameters
- Update only live-safe parameters to avoid pipeline reloads (confirm via Scope API docs).
- Start with prompt, guidance, delta, ControlNet scale if supported.
- Send webcam input with `pc.addTrack(track, inputStream)` before creating the offer.
- Send `reset_cache: true` when applying prompt/VACE changes to refresh the output.

## UI Requirements
- Minimal control panel:
  - Start/stop
  - Preset selector
  - Status indicator
  - Apply updates to push prompt/VACE changes to the active stream
- Output viewer for the live stream.
- Home focus selector to swap between Soundscape and Avatar Studio.
- Apply Updates stays disabled until a stream is live.
- Status messaging calls out reconnect attempts and failure states.

## Performance Targets
- Warm-up time: under 30s.
- Stable output without frequent resets.
- Latency low enough for a conversational demo.

## Demo Requirements
- 1-minute narrative: "Webcam in, Scope out, MetaDJ embodied."
- Single click to start once configured.
- Fallback plan if RunPod or local GPU fails.

## Future Enhancements
- Masking/segmentation for clean overlay (SAM3 or similar).
- Background environment layer generated by Scope.
- TouchDesigner or Spout pipeline for live performance control.

## Open Questions
- VACE input format and latency impact?
- Does Scope expose segmentation or alpha output?
