# Ideas - MetaDJ Scope

**Last Modified**: 2025-12-26 12:22 EST
**Status**: Selected Direction

## Selected Idea: MetaDJ Avatar - Live AI Persona (Scope-Generated)

### One-Line Summary
Real-time MetaDJ avatar generated by Scope from live webcam input, locked to identity via VACE.

### Target User
Stream viewers and demo judges watching a live, AI-generated MetaDJ persona.

### Scope Features Used
- StreamDiffusion real-time pipeline
- VACE for reference image consistency
- ControlNet (OpenPose + optional depth/edges) for pose guidance

### MVP Flow
1. Load MetaDJ reference images into VACE.
2. Ingest webcam frames as the motion/pose source.
3. Scope generates a consistent MetaDJ avatar in real time.
4. Output shown in a simple demo viewer (or OBS if needed).

### Constraints
- Keep the pipeline inside Scope (Scope is the avatar generator).
- Optimize for latency and stability over experimentation.
- Single, clear demo loop that runs reliably.

### Risks
- VACE consistency or latency not good enough for real-time feel.
- Webcam ingest path unclear (API vs desktop app).

### Win Factor
Clear story: "Webcam in, Scope out, MetaDJ persona embodied in real time."

---

## Future Add-Ons (Post-MVP)

### Masking / Segmentation (Streaming Overlay)
Goal: remove background and output clean alpha or mask for OBS overlays.
- Investigate Scope support for segmentation (SAM3 or similar).
- If supported: generate mask in Scope and composite.
- If not: keep as future extension or use downstream tooling.

### Background Environments
- Use Scope prompts to generate environment behind the avatar.
- Optionally separate layers for avatar vs background if feasible.

### TouchDesigner Integration (Optional)
- Only if Spout routing or live compositing adds real value.
- Not required for the MVP.

---

## Next Steps
- Validate VACE input format and webcam ingest path.
- Run a minimal Scope stream with reference images.
- Document MVP requirements in `docs/features/avatar-mvp-spec.md`.
